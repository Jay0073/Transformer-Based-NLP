{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Domain-Specific Token Classification Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import wget\n",
    "\n",
    "# set data path\n",
    "DATA_DIR = \"data/NCBI\"\n",
    "os.makedirs(DATA_DIR, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(f'{DATA_DIR}/NCBI_corpus_testing.txt') as f: \n",
    "    sample_text=f.readline()\n",
    "    \n",
    "print(sample_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "# use regular expression to find labels\n",
    "categories=re.findall('<category.*?<\\/category>', sample_text)\n",
    "for sample in categories: \n",
    "    print(sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NER_DATA_DIR = f'{DATA_DIR}/NER'\n",
    "os.makedirs(os.path.join(DATA_DIR, 'NER'), exist_ok=True)\n",
    "\n",
    "# show downloaded files\n",
    "!ls -lh $NER_DATA_DIR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# preview dataset\n",
    "!head -n 1 $NER_DATA_DIR/text_train.txt\n",
    "!head -n 1 $NER_DATA_DIR/labels_train.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "configuration file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define config path\n",
    "MODEL_CONFIG = \"token_classification_config.yaml\"\n",
    "WORK_DIR = \"WORK_DIR\"\n",
    "os.makedirs(WORK_DIR, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# download the model's configuration file \n",
    "BRANCH = 'main'\n",
    "config_dir = WORK_DIR + '/configs/'\n",
    "os.makedirs(config_dir, exist_ok=True)\n",
    "\n",
    "if not os.path.exists(config_dir + MODEL_CONFIG):\n",
    "    print('Downloading config file...')\n",
    "    wget.download(f'https://raw.githubusercontent.com/NVIDIA/NeMo/{BRANCH}/examples/nlp/token_classification/conf/' + MODEL_CONFIG, config_dir)\n",
    "else:\n",
    "    print ('config file already exists')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from omegaconf import OmegaConf\n",
    "\n",
    "CONFIG_DIR = \"/dli/task/WORK_DIR/configs\"\n",
    "CONFIG_FILE = \"token_classification_config.yaml\"\n",
    "\n",
    "config=OmegaConf.load(CONFIG_DIR + \"/\" + CONFIG_FILE)\n",
    "\n",
    "# print the entire configuration file\n",
    "print(OmegaConf.to_yaml(config))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# in this exercise, train and dev datasets are located in the same folder under the default names, \n",
    "# so it is enough to add the path of the data directory to the config\n",
    "config.model.dataset.data_dir = os.path.join(DATA_DIR, 'NER')\n",
    "\n",
    "# print the model section\n",
    "print(OmegaConf.to_yaml(config.model))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import dependencies\n",
    "from nemo.collections.nlp.models.language_modeling.megatron_bert_model import MegatronBertModel\n",
    "\n",
    "# list available pre-trained models\n",
    "for model in MegatronBertModel.list_available_models(): \n",
    "    print(model.pretrained_model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add the specified above model parameters to the config\n",
    "MODEL_NAME='biomegatron345m_biovocab_30k_cased'\n",
    "# MODEL_NAME='biomegatron-bert-345m-cased'\n",
    "\n",
    "config.model.language_model.lm_checkpoint=None\n",
    "config.model.language_model.pretrained_model_name=MODEL_NAME\n",
    "config.model.tokenizer.tokenizer_name=None\n",
    "\n",
    "# use appropriate configurations based on GPU capacity\n",
    "config.model.dataset_max_seq_length=64\n",
    "config.model.train_ds.batch_size=32\n",
    "config.model.validation_ds.batch_size=32\n",
    "config.model.test_ds.batch_size=32\n",
    "\n",
    "# limit the number of epochs for this demonstration\n",
    "config.trainer.max_epochs=1\n",
    "# config.trainer.precision=16\n",
    "# config.trainer.amp_level='O1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create trainer and model instances\n",
    "from nemo.collections.nlp.models import TokenClassificationModel\n",
    "import pytorch_lightning as pl\n",
    "\n",
    "trainer=pl.Trainer(**config.trainer)\n",
    "ner_model=TokenClassificationModel(cfg=config.model, trainer=trainer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# start model training\n",
    "trainer.fit(ner_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate model performance on sample\n",
    "ner_model.half().evaluate_from_file(\n",
    "    text_file=os.path.join(NER_DATA_DIR, 'sample_text_dev.txt'),\n",
    "    labels_file=os.path.join(NER_DATA_DIR, 'sample_labels_dev.txt'),\n",
    "    output_dir=WORK_DIR,\n",
    "    add_confusion_matrix=True,\n",
    "    normalize_confusion_matrix=True,\n",
    "    batch_size=1\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
